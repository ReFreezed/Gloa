--[[============================================================
--=
--=  Lexer
--=
--=-------------------------------------------------------------
--=
--=  Glóa - a language that compiles into Lua
--=  by Marcus 'ReFreezed' Thunström
--=
--==============================================================

	getToken, isToken, isTokenLiteral, isTokenBuiltinType, isTokenAssigning
	readAndTokenizeFile
	tokenize, insertToken

--============================================================]]

!(
local BYTE_NL        = ("\n"):byte()
local BYTE_CR        = ("\r"):byte()
local BYTE_TAB       = ("\t"):byte()
local BYTE_BACKSLASH = ("\\"):byte()

local keywords = Set{
	"and",
	"break",
	"case",
	"cast",
	"continue",
	"defer",
	"do",
	"else",
	"elseif", -- This actually gets split up into 'else if' or 'else !if'.
	"enum",
	"export",
	"external",
	"for",
	"if",
	"in",
	"inline",
	"local",
	"namespace",
	"not",
	"or",
	"read_only",
	"return",
	"static",
	"struct",
	"type_info",
	"type_of",
	"using",
	"variant_of",
	"while",
	-- Reserved words:
	"goto",
	"until", -- Should we have do-until?
	-- Literals:
	"false",
	"nil",
	"NULL", -- NULL should be used in situations where the actual value doesn't matter, as a dummy value.
	"true",
	-- Built-in types:
	"any",
	"bool",
	"float",
	"int",
	"string",
	"table",
	"type",
	"void", -- Like nil represents the absence of a value, void represents the absence of a type (or anything at all as all things must have a type).
	"none",
}

local directiveWords = Set{
	"assert",         -- For static/compile-time asserts. Can exist in both imperative and declarative scopes.
	"bake_arguments", -- For "baking" function arguments.
	"body_text",      -- For generating the body of a function at compile-time.
	"call",           -- For defining what happens when calling a struct.
	"char",           -- For getting the Unicode codepoint for a character.
	"complete",       -- For making sure every value of an enum is handled in a switch statement.
	"foreign",        -- For externally-defined values.
	"if",             -- For static/compile-time if statements. Can exist in both imperative and declarative scopes.
	"import",         -- For importing modules/libraries from a common folder. (Related to !load)
	"key",            -- For strongly typed table structs. (Related to !value)
	"load",           -- For loading .gloa files by path. (Related to !import)
	"must",           -- For return arguments that must be received by the caller (e.g. file handles).
	"preload",        -- For running Lua code before any Glóa code runs.
	"print",          -- For printing out what some expression refers to during compilation. (For debugging)
	"run",            -- For running code at compile-time.
	"shadow",         -- For allowing name shadowing (which is normally disallowed) for a declared name.
	"through",        -- For joining cases in switch statements.
	"value",          -- For strongly typed table structs or array-like structs.
	-- May or may not implement:
	"modify",         -- For modifying declared functions and structs. (Related to !body_text)
	"operator",       -- For overloading operators.
}
)

local KEYWORDS        = !(keywords)
local DIRECTIVE_WORDS = !(directiveWords)



_G.!struct"Tokens"{ -- SOA
	{`count`,     0},

	{`type`,      {}}, -- One of TOKEN_*.
	{`value`,     {}}, -- Depends on the type.
	{`extra`,     {}}, -- Depends on the type.
	{`file`,      {}},
	{`position1`, {}}, -- Start byte.
	{`position2`, {}}, -- End byte.
	{`line1`,     {}}, -- Start line.
	{`line2`,     {}}, -- End line.
	{`inserted`,  {}}, -- Used for compilation stats.

	{`lineCount`, 0},
}

function _G.tokenize(s, path, tokens, increaseTotalLineCount)
	local getByte = string.byte

	local tokTypes      = tokens.type
	local tokValues     = tokens.value
	local tokExtras     = tokens.extra
	local tokFiles      = tokens.file
	local tokPositions1 = tokens.position1
	local tokPositions2 = tokens.position2
	local tokLines1     = tokens.line1
	local tokLines2     = tokens.line2

	local ptr   = 1
	local ln    = 1
	local lnPtr = 1

	local forceIdentifier = false

	local function updateLineNumber()
		while lnPtr < ptr do
			if getByte(s, lnPtr) == !(getByte"\n") then
				ln = ln+1
			end
			lnPtr = lnPtr+1
		end
	end

	while true do
		-- Ignore whitespace.
		local i1, i2 = s:find("^%s+", ptr)
		if i1 then
			ptr = i2+1
			updateLineNumber()
		end

		if ptr > #s then  break  end -- EOF

		local tokenPos = ptr
		local tokType, tokValue, tokExtra = nil

		-- Identifier/keyword/blank.
		if s:find("^[%a_\194-\244]", ptr) then
			local i1 = ptr
			local i2, _

			for i = 1, math.huge do
				local byte = getByte(s, ptr)
				if not byte then
					break -- EOF
				elseif byte <= 127 then
					_, i2 = s:find("^[%w_]+", ptr)
				else
					_, i2 = s:find("^[\194-\244][\128-\191]*", ptr)
				end

				if i2 then
					ptr = i2+1
				else
					break
				end
			end

			local word = s:sub(i1, ptr-1)

			if forceIdentifier then
				forceIdentifier = false
				tokType  = !(TOKEN_IDENTIFIER)
				tokValue = word
			elseif word == "_" then
				tokType  = !(TOKEN_PUNCTUATION)
				tokValue = "_"
			elseif word == "elseif" then
				-- Split up 'elseif' into 'else if'.
				tokType  = !(TOKEN_KEYWORD)
				tokValue = "else"
				ptr      = ptr-2
			else
				tokType  = KEYWORDS[word] and !(TOKEN_KEYWORD) or !(TOKEN_IDENTIFIER)
				tokValue = word
			end

		elseif forceIdentifier then
			errorInFile(s, path, ptr, "Tokenizer", "Expected a word after '\\'.")

		-- Escaped identifier.
		elseif s:find("^\\", ptr) then
			ptr             = ptr+1
			forceIdentifier = true
			tokType         = nil

		-- Directive.
		elseif s:find("^!%a", ptr) then
			ptr = ptr+1

			local i1, i2, word = s:find("^(%a+)", ptr)
			if not DIRECTIVE_WORDS[word] then
				errorInFile(s, path, ptr, "Tokenizer", "Unknown directive '%s'.", word)
			end
			ptr = i2+1

			tokType  = !(TOKEN_DIRECTIVE)
			tokValue = word

		-- Number (hexadecimal, int/float).
		elseif s:find("^0[Xx]", ptr) then
			!(
			local NUM_HEX_FRAC_EXP_1 = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_EXP_2 = ("^( .. _*                          %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_1     = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_2     = ("^( .. _*                          %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_EXP        = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*)                  [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX            = ("^( .. _*  [%dA-Fa-f][%dA-Fa-f_]*                                                     )"):gsub(" +", "") -- int
			)
			local           lua52Hex, isInt, i1, i2, numStr = true,  false, s:find(!(NUM_HEX_FRAC_EXP_1), ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, s:find(!(NUM_HEX_FRAC_EXP_2), ptr)  end
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, s:find(!(NUM_HEX_FRAC_1),     ptr)  end
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, s:find(!(NUM_HEX_FRAC_2),     ptr)  end
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, s:find(!(NUM_HEX_EXP),        ptr)  end
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = false, true,  s:find(!(NUM_HEX),            ptr)  end

			if not numStr then
				errorInFile(s, path, ptr, "Tokenizer", "Malformed hexadecimal number.")
			end

			numStr  = numStr:gsub("_+", "")
			local n = tonumber(numStr)

			-- Support hexadecimal floats in Lua 5.1.
			if not n and lua52Hex then
				local               _, intStr, fracStr, expStr = numStr:match(!(NUM_HEX_FRAC_EXP_1))
				if not intStr then  _,         fracStr, expStr = numStr:match(!(NUM_HEX_FRAC_EXP_2)) ; intStr                  = ""           end
				if not intStr then  _, intStr, fracStr         = numStr:match(!(NUM_HEX_FRAC_1))     ;                  expStr =         "0"  end
				if not intStr then  _,         fracStr         = numStr:match(!(NUM_HEX_FRAC_2))     ; intStr,          expStr = "",     "0"  end
				if not intStr then  _, intStr,          expStr = numStr:match(!(NUM_HEX_EXP))        ;         fracStr         =     ""       end
				assert(intStr, numStr)

				n = tonumber(intStr, 16) or 0 -- Note: intStr may be "".

				local fracValue = 1
				for i = 1, #fracStr do
					fracValue = fracValue / 16
					n         = n + tonumber(fracStr:sub(i, i), 16) * fracValue
				end

				n = n * 2^expStr:gsub("^+", "")
			end

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid hexadecimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- Number (decimal, int/float).
		elseif s:find("^%.?%d", ptr) then
			!(
			local NUM_DEC_FRAC_EXP_1 = ("^( %d[%d_]* %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_EXP_2 = ("^(          %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_1     = ("^( %d[%d_]* %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_2     = ("^(          %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_EXP        = ("^( %d[%d_]*             [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC            = ("^( %d[%d_]*                               )"):gsub(" +", "") -- int
			)
			local           isInt, i1, i2, numStr = false, s:find(!(NUM_DEC_FRAC_EXP_1), ptr)
			if not i1 then  isInt, i1, i2, numStr = false, s:find(!(NUM_DEC_FRAC_EXP_2), ptr)  end
			if not i1 then  isInt, i1, i2, numStr = false, s:find(!(NUM_DEC_FRAC_1),     ptr)  end
			if not i1 then  isInt, i1, i2, numStr = false, s:find(!(NUM_DEC_FRAC_2),     ptr)  end
			if not i1 then  isInt, i1, i2, numStr = false, s:find(!(NUM_DEC_EXP),        ptr)  end
			if not i1 then  isInt, i1, i2, numStr = true,  s:find(!(NUM_DEC),            ptr)  end

			if not numStr then
				errorInFile(s, path, ptr, "Tokenizer", "Malformed decimal number.")
			end

			numStr  = numStr:gsub("_+", "")
			local n = tonumber(numStr)

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid decimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- String (short-form).
		elseif s:find("^[\"']", ptr) then
			local reprStart = ptr
			local quoteByte = getByte(s, ptr)
			local bytes     = {} -- @Speed: Reuse this table for all strings.

			ptr = ptr+1

			while true do
				local byte = getByte(s, ptr)

				if not byte then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string.")

				-- End of string.
				elseif byte == quoteByte then
					ptr = ptr+1
					break

				-- Escape sequence.
				elseif byte == !(BYTE_BACKSLASH) then
					ptr  = ptr+1
					byte = getByte(s, ptr)

					if not byte then
						errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string after escape character.")

					-- \n or \ followed by a newline inserts a newline.
					-- \r inserts a carriage return,
					-- \t inserts a horizontal tab.
					-- \\ inserts a backslash.
					-- \" inserts a quotation mark.
					-- \' inserts an apostrophe.
					elseif byte == !(getByte"n") then
						byte = !(BYTE_NL)
						ptr  = ptr+1
					elseif byte == !(getByte"r") then
						byte = !(BYTE_CR)
						ptr  = ptr+1
					elseif byte == !(getByte"t") then
						byte = !(BYTE_TAB)
						ptr  = ptr+1
					elseif
						byte == !(BYTE_BACKSLASH) or byte == !(BYTE_NL) or
						byte == !(getByte'"')     or byte == !(getByte"'")
					then
						-- Keep value of 'byte'.
						ptr = ptr+1

					-- \ddd specifies a byte denoted by one to three decimal digits.
					elseif s:find("^%d", ptr) then
						local numStr = s:match("^%d%d?%d?", ptr)
						byte         = tonumber(numStr)
						if byte > 255 then
							errorInFile(
								s, path, ptr, "Tokenizer",
								"Invalid escape sequence. (Decimal number '%d' does not fit inside a byte.)",
								byte
							)
						end
						ptr = ptr + #numStr

					-- \xXX specifies a byte denoted by two hexadecimal digits.
					elseif s:find("^x[%dA-Fa-f][%dA-Fa-f]", ptr) then
						local hex = s:sub(ptr+1, ptr+2)
						byte      = tonumber(hex, 16)
						ptr       = ptr+3

					-- \u{XXX} inserts the UTF-8 encoding of a Unicode codepoint denoted by a sequence of hexadecimal digits.
					elseif s:find("^u{[%dA-Fa-f]+}", ptr) then
						local hex = s:match("^[%dA-Fa-f]+", ptr+2)
						local cp  = tonumber(hex, 16)
						byte      = nil
						ptr       = ptr + 3 + #hex

						local cpStr = utf8Char(cp)
						for i = 1, #cpStr do
							table.insert(bytes, getByte(cpStr, i))
						end

					-- \z skips the following span of whitespace characters.
					elseif byte == !(getByte"z") then
						local i1, i2 = s:find("^%s*", ptr+1)
						byte = nil
						ptr  = i2+1

					else
						errorInFile(s, path, ptr-1, "Tokenizer", "Invalid escape sequence.")
					end

					if byte then
						table.insert(bytes, byte)
					end

				-- Illegal characters.
				elseif byte == !(BYTE_NL) then
					errorInFile(s, path, ptr, "Tokenizer", "Invalid string. (Line breaks must be escaped inside strings.)")

				-- Any other character.
				else
					table.insert(bytes, byte)
					ptr = ptr+1
				end
			end

			tokType  = !(TOKEN_STRING)
			tokValue = string.char(unpack(bytes)) -- @Robustness: Make sure this works for very long strings.
			tokExtra = false--trimmedInitialNewline

		-- String (long-form).
		elseif s:find("^%[=*%[", ptr) then
			local reprStart      = ptr
			local longEqualSigns = s:match("^%[(=*)%[", ptr)

			ptr = ptr + 2 + #longEqualSigns

			local stringPos1            = ptr
			local trimmedInitialNewline = false

			if getByte(s, stringPos1) == !(BYTE_NL) then
				stringPos1            = stringPos1+1
				trimmedInitialNewline = true
			end

			local i1, i2 = s:find("%]"..longEqualSigns.."%]", ptr)
			if not i1 then
				errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long string.")
			end
			ptr = i2+1

			local stringPos2 = i1-1

			tokType  = !(TOKEN_STRING)
			tokValue = s:sub(stringPos1, stringPos2)
			tokExtra = trimmedInitialNewline

		-- Comment.
		elseif s:find("^%-%-", ptr) then
			local reprStart = ptr
			ptr = ptr+2

			local longEqualSigns = s:match("^%[(=*)%[", ptr)

			-- Single line.
			if not longEqualSigns then
				local i = s:find("\n", ptr)
				ptr     = (i or #s) + 1

			-- Long-form.
			else
				ptr = ptr + 2 + #longEqualSigns

				local i1, i2 = s:find("%]"..longEqualSigns.."%]", ptr)
				if not i1 then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long comment.")
				end
				ptr = i2+1
			end

			-- Don't set tokType - just ignore the comment!

		-- Punctuation.
		elseif
			s:find("^//=",            ptr) or
			s:find("^%.%.=",          ptr) or
			s:find("^%.%.%.",         ptr)
		then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = s:sub(ptr, ptr+2)
			ptr      = ptr+3
		elseif
			s:find("^//",             ptr) or
			s:find("^<<",             ptr) or
			s:find("^>>",             ptr) or
			s:find("^%->",            ptr) or
			s:find("^%.%.",           ptr) or
			s:find("^[-+*/%%^=~<>]=", ptr)
		then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = s:sub(ptr, ptr+1)
			ptr      = ptr+2
		elseif
			s:find("^[-+*/%%^#<>=(){}[%];:,.|$!?]", ptr)
		then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = s:sub(ptr, ptr)
			ptr      = ptr+1

		else
			errorInFile(s, path, ptr, "Tokenizer", "Unknown character. (Byte: %d)", getByte(s, ptr))
		end

		if tokType then
			tokens.count = tokens.count+1

			tokTypes[tokens.count]      = tokType
			tokValues[tokens.count]     = tokValue
			tokExtras[tokens.count]     = tokExtra
			tokFiles[tokens.count]      = path
			tokPositions1[tokens.count] = tokenPos
			tokPositions2[tokens.count] = ptr-1
			tokLines1[tokens.count]     = ln
		end

		updateLineNumber()

		if tokType then
			tokLines2[tokens.count] = ln
			--[[
			printf("%5d %s%s |%s|",
				#tokTypes,
				TOKEN_TITLES[tokType],
				(" "):rep(11-#TOKEN_TITLES[tokType]),
				tostring(tokValue))
			--]]
		end
	end

	if increaseTotalLineCount then
		tokens.lineCount = tokens.lineCount+ln
	end
end

-- insertToken( tokens, tokenType, tokenValue [, path=tokens.file[1] ] )
function _G.insertToken(tokens, tokType, tokValue, path)
	local count             = tokens.count+1
	tokens.count            = count
	tokens.type [count]     = tokType
	tokens.value[count]     = tokValue
	tokens.position1[count] = 1
	tokens.position2[count] = 1
	tokens.line1[count]     = 1
	tokens.line2[count]     = 1
	tokens.file[count]      = path or tokens.file[1]
	tokens.inserted[count]  = true
end



-- tokenType, tokenValue = getToken( tokens, index )
function _G.getToken(tokens, i)
	return tokens.type[i], tokens.value[i]
end

-- bool = isToken( targetTokenType, targetTokenValue, tokType [, tokValue1, ... ] )
function _G.isToken(targetTokType, targetTokValue, tokType, ...)
	if targetTokType ~= tokType then  return false  end

	local varargCount = select("#", ...)
	if varargCount == 0 then  return true  end

	for i = 1, varargCount do
		if targetTokValue == select(i, ...) then  return true  end
	end

	return false
end

function _G.isTokenLiteral(tokType, tokValue)
	return
		!!(anyV(`tokType`, TOKEN_FLOAT,TOKEN_INTEGER,TOKEN_STRING))
		or isToken(tokType,tokValue, !(TOKEN_KEYWORD),"true","false","nil")
end

function _G.isTokenBuiltinType(tokType, tokValue)
	return isToken(tokType,tokValue, !(TOKEN_KEYWORD),"float","int","string","table","bool","type","any","none")
end

function _G.isTokenAssigning(tokType, tokValue)
	return isToken(tokType,tokValue, !(TOKEN_PUNCTUATION),"=","+=","-=","*=","/=","//=","^=","%=","..=")
end



do
	local BRACKETS = {["("]=")", ["{"]="}", ["["]="]"}

	function _G.readAndTokenizeFile(state, path, increaseTotalLineCount)
		!ifDEBUG `profilerPause()`
		assert(not state.fileBuffers[path], path)

		local s, err = getFileContents(path, true)
		if not s then
			errorLine(nil, "Could not read file '%s'. (%s)", path, err)
		end
		s = s:gsub("\r\n?", "\n") -- Normalize line endings.  @Robustness: Handle the uncommon occurrence of \n\r.

		state.fileBuffers[path] = s

		local tokens     = state.tokens
		local startToken = tokens.count+1

		local startTime = os.clock()
		tokenize(s, path, tokens, increaseTotalLineCount)
		local endTime = os.clock()

		-- Quickly check for stray/unmatched brackets.

		-- Note: Sometimes an error message here isn't very helpful, so we
		-- sometimes continue with parsing despite getting an error here.

		local bracketTokenIndices = {}

		for i = startToken, tokens.count do
			if isToken(tokens.type[i],tokens.value[i], !(TOKEN_PUNCTUATION),"(","{","[") then
				table.insert(bracketTokenIndices, i)

			elseif isToken(tokens.type[i],tokens.value[i], !(TOKEN_PUNCTUATION),")","}","]") then
				local iStart   = table.remove(bracketTokenIndices)
				local expected = BRACKETS[tokens.value[iStart]]

				if not iStart then
					reportMessageInFile(io.stderr, s, path, tokens.position1[i], "Error", "BracketChecker", "Stray bracket.")
					bracketTokenIndices[1] = nil
					break

				elseif tokens.value[i] ~= expected then
					printerr()
					reportMessageInFile(io.stderr, s, path, tokens.position1[i],      "Error", "BracketChecker", "Expected '%s'.", expected)
					reportMessageInFile(io.stderr, s, path, tokens.position1[iStart], "Info",  "BracketChecker", "...here is the unmatched bracket.")
					exitFailure()
				end
			end
		end

		if bracketTokenIndices[1] then
			local i = bracketTokenIndices[1]
			errorInFile(s, path, tokens.position1[i], "BracketChecker", "Missing end bracket.", BRACKETS[tokens.value[i]])
		end

		!ifDEBUG `profilerUnpause()`
		return startTime, endTime
	end
end


