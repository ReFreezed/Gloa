--[[============================================================
--=
--=  Lexer
--=
--=-------------------------------------------------------------
--=
--=  Glóa - a language that compiles into Lua
--=  by Marcus 'ReFreezed' Thunström
--=
--==============================================================

	getToken, isTokenLiteral, isTokenBuiltinType, isTokenAssigning
	readAndTokenizeString, readAndTokenizeFile
	tokenize, insertToken

--============================================================]]

!(
local BYTE_SPACE     = getByte" "
local BYTE_NL        = getByte"\n"
local BYTE_CR        = getByte"\r"
local BYTE_TAB       = getByte"\t"
local BYTE_BACKSLASH = getByte"\\"

-- Keyword notes:
--   elseif: This actually gets split up into 'else if' or 'else !if'.
--   NULL:   NULL should be used in situations where the actual value doesn't matter, as a dummy value.
--   void:   Like nil represents the absence of a value, void represents the absence of a type (or anything at all as all things must have a type).
local KEYWORDS = {
	["and"]          = true,
	["break"]        = true,
	["case"]         = true,
	["cast"]         = true,
	["continue"]     = true,
	["defer"]        = true,
	["do"]           = true,
	["else"]         = true,
	["elseif"]       = true,
	["enum"]         = true,
	["export"]       = true,
	["export_value"] = true,
	["external"]     = true,
	["for"]          = true,
	["if"]           = true,
	["in"]           = true,
	["inline"]       = true,
	["local"]        = true,
	["namespace"]    = true,
	["not"]          = true,
	["or"]           = true,
	["return"]       = true,
	["static"]       = true,
	["struct"]       = true,
	["type_info"]    = true,
	["type_of"]      = true,
	["using"]        = true,
	["variant_of"]   = true,
	["while"]        = true,
	-- Reserved words:
	["goto"]         = true,
	["read_only"]    = true,
	["until"]        = true,
	-- Literals:
	["false"]        = true,
	["nil"]          = true,
	["NULL"]         = true,
	["true"]         = true,
	-- Built-in types:
	["any"]          = true,
	["bool"]         = true,
	["float"]        = true,
	["int"]          = true,
	["none"]         = true,
	["string"]       = true,
	["table"]        = true,
	["Type"]         = true,
	["void"]         = true,
}

local DIRECTIVE_WORDS = Set{
	"assert",         -- For static/compile-time asserts. Can exist in both imperative and declarative scopes.
	"bake_arguments", -- For "baking" function arguments.
	"body_text",      -- For generating the body of a function at compile-time.
	"call",           -- For defining what happens when calling a struct.
	"caller_location",-- For getting the source code location of a call.
	"char",           -- For getting the Unicode codepoint for a character.
	"compiler",       -- For compiler-provided literal values.
	"complete",       -- For making sure every value of an enum is handled in a switch statement.
	"foreign",        -- For externally-defined values.
	"if",             -- For static/compile-time if statements. Can exist in both imperative and declarative scopes.
	"import",         -- For importing modules/libraries from a common folder. (Related to !load)
	"iterator",       -- For defining default iterators for structs.
	"key",            -- For table-like structs. (Related to !value)
	"load",           -- For loading Glóa files by path. (Related to !import)
	"location",       -- For getting a source code location.
	"must",           -- For return arguments that must be received by the caller (e.g. file handles).
	"preload",        -- For running Lua code before any Glóa code runs.
	"print",          -- For printing out what some expression refers to during compilation. (For debugging)
	"run",            -- For running code at compile-time.
	"self",           -- For a file to reference itself.
	"shadow",         -- For allowing name shadowing (which is normally disallowed) for a declared name.
	"through",        -- For joining cases in switch statements.
	"value",          -- For table-like or array-like structs.
	-- May or may not implement:
	"modify",         -- For modifying declared functions and structs. (Related to !body_text)
	"operator",       -- For overloading operators.
}
)

local KEYWORDS        = !(KEYWORDS)
local DIRECTIVE_WORDS = !(DIRECTIVE_WORDS)



_G.!struct"Tokens"{ -- SOA
	{`count`,     0},

	{`type`,      {}}, -- One of TOKEN_*.
	{`value`,     {}}, -- Depends on the type.
	{`extra`,     {}}, -- Depends on the type.
	{`file`,      {}},
	{`buffer`,    {}}, -- The actual source string (which may not be from the file).
	{`position1`, {}}, -- Start byte.
	{`position2`, {}}, -- End byte.
	{`line1`,     {}}, -- Start line.
	{`line2`,     {}}, -- End line.
	{`inserted`,  {}}, -- Used for compilation stats.

	{`lineCount`, 0},
}

function _G.tokenize(s, path, tokens, increaseTotalLineCount, lnStart)
	local HUGE = math.huge

	local bytesToString = string.char
	local find          = string.find
	local getByte       = string.byte
	local gsub          = string.gsub
	local match         = string.match
	local sub           = string.sub
	local tonumber      = tonumber
	local utf8Char      = utf8Char

	local tokTypes      = tokens.type
	local tokValues     = tokens.value
	local tokExtras     = tokens.extra
	local tokFiles      = tokens.file
	local tokBuffers    = tokens.buffer
	local tokPositions1 = tokens.position1
	local tokPositions2 = tokens.position2
	local tokLines1     = tokens.line1
	local tokLines2     = tokens.line2

	local count = tokens.count

	local ptr   = 1
	local ln    = lnStart
	local lnPtr = 1
	local lnPos = nil

	local expectWordAfterEscape = false

	!local UPDATE_LINE_NUMBER = `
		while true do
			if not lnPos then
				lnPos = find(s, "\n", lnPtr, true)

				if not lnPos then
					lnPtr = HUGE
					break
				end
			end

			if lnPos >= ptr then
				lnPtr = ptr-1
				break
			end

			ln    = ln+1
			lnPtr = lnPos+1
			lnPos = nil
		end
	`

	while true do
		local byte = getByte(s, ptr)

		-- Ignore whitespace.
		if (byte == !(BYTE_SPACE) or byte == !(BYTE_NL) or byte == !(BYTE_TAB)) and not expectWordAfterEscape then
			local _, i2 = find(s, "^%s*", ptr+1)
			ptr         = i2+1
			byte        = getByte(s, ptr)

			!!(UPDATE_LINE_NUMBER)
		end

		if ptr > #s then  break  end -- EOF

		local tokenPos = ptr
		local tokType, tokValue, tokExtra = nil

		-- Identifier/keyword/blank.
		!(
		local bytes = {getByte"_"}
		for byte = getByte"A", getByte"Z" do  table.insert(bytes, byte)  end
		for byte = getByte"a", getByte"z" do  table.insert(bytes, byte)  end
		for byte = 194,        244        do  table.insert(bytes, byte)  end
		)
		if !!(CONST_SET(bytes))[byte] then
			local i2

			for _ = 1, HUGE do
				byte = getByte(s, ptr)
				if not byte then
					break -- EOF
				elseif byte <= 127 then
					_, i2 = find(s, "^[%w_]+", ptr)
				else
					_, i2 = find(s, "^[\194-\244][\128-\191]*", ptr)
				end

				if i2 then
					ptr = i2+1
				else
					break
				end
			end

			local word = sub(s, tokenPos, ptr-1)

			if expectWordAfterEscape then
				expectWordAfterEscape = false
				tokType  = !(TOKEN_IDENTIFIER)
				tokValue = word
			elseif word == "_" then
				tokType  = !(TOKEN_PUNCTUATION)
				tokValue = "_"
			elseif word == "elseif" then
				-- Split up 'elseif' into 'else if'.
				tokType  = !(TOKEN_KEYWORD)
				tokValue = "else"
				ptr      = ptr-2
			else
				tokType  = KEYWORDS[word] and !(TOKEN_KEYWORD) or !(TOKEN_IDENTIFIER)
				tokValue = word
			end

		elseif expectWordAfterEscape then
			errorInFile(s, path, ptr, "Tokenizer", "Expected a word after '\\'.")

		-- Number (hexadecimal, int/float).
		elseif byte == !(getByte"0") and find(s, "^[Xx]", ptr+1) then
			!(
			local NUM_HEX_FRAC_EXP_1 = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_EXP_2 = ("^( .. _*                          %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_1     = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_2     = ("^( .. _*                          %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_EXP        = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*)                  [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX            = ("^( .. _*  [%dA-Fa-f][%dA-Fa-f_]*                                                     )"):gsub(" +", "") -- int
			)
			local           lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_EXP_1), ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_EXP_2), ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_1),     ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_2),     ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_EXP),        ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = false, true,  find(s, !(NUM_HEX),            ptr)
			if not i1 then  errorInFile(s, path, ptr, "Tokenizer", "Malformed hexadecimal number.")
			end end end end end end

			if find(numStr, "_", 1, true) then  numStr = gsub(numStr, "_+", "")  end
			local n = tonumber(numStr)

			-- Support hexadecimal floats in Lua 5.1.
			if not n and lua52Hex then
				local               _, intStr, fracStr, expStr = match(numStr, !(NUM_HEX_FRAC_EXP_1))
				if not intStr then  _,         fracStr, expStr = match(numStr, !(NUM_HEX_FRAC_EXP_2)) ; intStr                  = ""
				if not intStr then  _, intStr, fracStr         = match(numStr, !(NUM_HEX_FRAC_1))     ;                  expStr =         "0"
				if not intStr then  _,         fracStr         = match(numStr, !(NUM_HEX_FRAC_2))     ; intStr,          expStr = "",     "0"
				if not intStr then  _, intStr,          expStr = match(numStr, !(NUM_HEX_EXP))        ;         fracStr         =     ""
				assert(intStr, numStr)
				end end end end

				n = tonumber(intStr, 16) or 0 -- Note: intStr may be "".

				local fracValue = 1
				for i = 1, #fracStr do
					fracValue = fracValue / 16
					n         = n + tonumber(sub(fracStr, i, i), 16) * fracValue
				end

				n = n * 2 ^ gsub(expStr, "^+", "")
			end

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid hexadecimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- Number (decimal, int/float).
		elseif (byte >= !(getByte"0") and byte <= !(getByte"9")) or (byte == !(getByte".") and find(s, "^%d", ptr+1)) then
			!(
			local NUM_DEC_FRAC_EXP_1 = ("^( %d[%d_]* %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_EXP_2 = ("^(          %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_1     = ("^( %d[%d_]* %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_2     = ("^(          %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_EXP        = ("^( %d[%d_]*             [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC            = ("^( %d[%d_]*                               )"):gsub(" +", "") -- int
			)
			local isInt = false
			local           i1, i2, numStr = find(s, !(NUM_DEC_FRAC_EXP_1), ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_EXP_2), ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_1),     ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_2),     ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_EXP),        ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC),            ptr) ; isInt = true
			if not i1 then  errorInFile(s, path, ptr, "Tokenizer", "Malformed decimal number.")
			end end end end end end

			if find(numStr, "_", 2, true) then  numStr = gsub(numStr, "_+", "")  end
			local n = tonumber(numStr)

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid decimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- String (short-form).
		elseif byte == !(getByte'"') or byte == !(getByte"'") then
			local reprStart = ptr
			local quoteByte = getByte(s, ptr)
			local segCount  = 0
			local pattern   = quoteByte == !(getByte'"') and '["\\]' or "['\\]"

			ptr = ptr + 1 -- The starting quote.

			!(
			preprocessorOutputAtTopOfFile `local substrings = {}` -- Reuse the same table for all strings.

			local function ADD(substrCode)
				__LUA(templateToLua(
					trimTemplate`
						segCount                 = segCount + 1
						substrings[segCount] = $substr
					`,
					{substr=substrCode}
				))
			end
			)

			while true do
				local i = find(s, pattern, ptr)
				if not i then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string.")
				end

				if i > ptr then
					!ADD `sub(s, ptr, i-1)`
				end
				ptr = i

				local byte = getByte(s, ptr)
				ptr        = ptr + 1 -- The matching special character.

				-- End of string.
				if byte == quoteByte then
					break

				-- Escape sequence.
				else--if byte == !(BYTE_BACKSLASH) then
					byte = getByte(s, ptr)

					if not byte then
						errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string after escape character.")

					-- \n or \ followed by a newline inserts a newline.
					-- \r inserts a carriage return,
					-- \t inserts a horizontal tab.
					-- \\ inserts a backslash.
					-- \" inserts a quotation mark.
					-- \' inserts an apostrophe.
					elseif byte == !(getByte"n") or byte == !(getByte"\n") then
						!ADD `"\n"`
						ptr = ptr + 1
					elseif byte == !(getByte"r") then
						!ADD `"\r"`
						ptr = ptr + 1
					elseif byte == !(getByte"t") then
						!ADD `"\t"`
						ptr = ptr + 1
					elseif byte == !(getByte"\\") then
						!ADD `"\\"`
						ptr = ptr + 1
					elseif byte == !(getByte'"') then
						!ADD `'"'`
						ptr = ptr + 1
					elseif byte == !(getByte"'") then
						!ADD `"'"`
						ptr = ptr + 1

					-- \ddd specifies a byte denoted by one to three decimal digits.
					elseif find(s, "^%d", ptr) then
						local numStr = match(s, "^%d%d?%d?", ptr)
						byte         = tonumber(numStr)
						if byte > 255 then
							errorInFile(
								s, path, ptr, "Tokenizer",
								"Invalid escape sequence. (Decimal number '%d' does not fit inside a byte.)",
								byte
							)
						end
						ptr = ptr + #numStr
						!ADD `bytesToString(byte)`

					-- \xXX specifies a byte denoted by two hexadecimal digits.
					elseif find(s, "^x[%dA-Fa-f][%dA-Fa-f]", ptr) then
						local hex = sub(s, ptr+1, ptr+2)
						ptr       = ptr + 3
						!ADD `bytesToString(tonumber(hex, 16))`

					-- \u{XXX} inserts the UTF-8 encoding of a Unicode codepoint denoted by a sequence of hexadecimal digits.
					elseif find(s, "^u{[%dA-Fa-f]+}", ptr) then
						-- @Robustness: Validate integer range.
						local hex = match(s, "^[%dA-Fa-f]+", ptr+2)
						ptr       = ptr + 3 + #hex
						!ADD `utf8Char(tonumber(hex, 16))`

					-- \z skips the following span of whitespace characters.
					elseif byte == !(getByte"z") then
						local i1, i2 = find(s, "^%s*", ptr+1)
						ptr          = i2 + 1

					else
						errorInFile(s, path, ptr-1, "Tokenizer", "Invalid escape sequence.")
					end
				end
			end

			tokType  = !(TOKEN_STRING)
			tokValue = table.concat(substrings, "", 1, segCount)
			tokExtra = false--trimmedInitialNewline

		-- String (long-form).
		elseif byte == !(getByte"[") and find(s, "^=*%[", ptr+1) then
			local reprStart      = ptr
			local longEqualSigns = match(s, "^%[(=*)%[", ptr)

			ptr = ptr + 2 + #longEqualSigns

			local stringPos1            = ptr
			local trimmedInitialNewline = false

			if getByte(s, stringPos1) == !(BYTE_NL) then
				stringPos1            = stringPos1+1
				trimmedInitialNewline = true
			end

			local i1, i2 = find(s, "%]"..longEqualSigns.."%]", ptr)
			if not i1 then
				errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long string.")
			end
			ptr = i2+1

			local stringPos2 = i1-1

			tokType  = !(TOKEN_STRING)
			tokValue = sub(s, stringPos1, stringPos2)
			tokExtra = trimmedInitialNewline

		-- Comment.
		elseif byte == !(getByte"-") and find(s, "^%-", ptr+1) then
			local reprStart = ptr
			ptr = ptr+2

			local longEqualSigns = match(s, "^%[(=*)%[", ptr)

			-- Single line.
			if not longEqualSigns then
				local i = find(s, "\n", ptr)
				ptr     = (i or #s) + 1

			-- Long-form.
			else
				ptr = ptr + 2 + #longEqualSigns

				local i1, i2 = find(s, "%]"..longEqualSigns.."%]", ptr)
				if not i1 then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long comment.")
				end
				ptr = i2+1
			end

			-- Don't set tokType - just ignore the comment!

		-- Directive.
		elseif byte == !(getByte"!") and find(s, "^%a", ptr+1) then
			ptr = ptr+1

			local i1, i2, word = find(s, "^(%a[%w_]+)", ptr)
			if not DIRECTIVE_WORDS[word] then
				errorInFile(s, path, ptr, "Tokenizer", "Unknown directive '%s'.", word)
			end
			ptr = i2+1

			tokType  = !(TOKEN_DIRECTIVE)
			tokValue = word

		-- Punctuation.
		!local BYTES_EQ = {
			getByte"+", getByte"-", getByte"*", getByte"/", getByte"%", getByte"^",
			getByte"=", getByte"~", getByte"<", getByte">",
		}
		!local BYTES_SINGLE = {
			getByte"+", getByte"-", getByte"*", getByte"/", getByte"%", getByte"^", getByte"#", getByte"=",
			getByte"<", getByte">", getByte"(", getByte")", getByte"{", getByte"}", getByte"[", getByte"]",
			getByte":", getByte";", getByte".", getByte",",
			getByte"|", getByte"$", getByte"!", getByte"?",
		}
		elseif byte == !(getByte".")         and (find(s, "^%.=", ptr+1) or find(s, "^%.%.", ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+2) ; ptr = ptr+3
		elseif byte == !(getByte"/")         and (find(s, "^/=",  ptr+1)                           ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+2) ; ptr = ptr+3
		elseif byte == !(getByte".")         and (find(s, "^%.",  ptr+1)                           ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif byte == !(getByte"/")         and (find(s, "^/",   ptr+1)                           ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif byte == !(getByte"-")         and (find(s, "^>",   ptr+1)                           ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif !!(CONST_SET(BYTES_EQ))[byte] and (find(s, "^=",   ptr+1)                           ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif !!(CONST_SET(BYTES_SINGLE))[byte]                                                     then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr  ) ; ptr = ptr+1

		elseif byte == !(getByte"@") then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = "@"
			ptr      = ptr+1
			if not find(s, "^[%a_\194-\244]", ptr) then
				errorInFile(s, path, ptr, "Tokenizer", "Expected name after '@'.")
			end

		-- Escaped identifier.
		elseif byte == !(getByte"\\") then
			ptr                   = ptr+1
			expectWordAfterEscape = true
			-- Don't set tokType - just proceed to the next token!

		else
			errorInFile(s, path, ptr, "Tokenizer", "Unknown character. (Byte: %d)", byte)
		end

		if tokType then
			count = count + 1

			tokTypes[count]      = tokType
			tokValues[count]     = tokValue
			tokFiles[count]      = path
			tokBuffers[count]    = s
			tokPositions1[count] = tokenPos
			tokPositions2[count] = ptr-1
			tokLines1[count]     = ln

			if tokExtra ~= nil then  tokExtras[count] = tokExtra  end -- The if is needed for speed!
		end

		!!(UPDATE_LINE_NUMBER)

		if tokType then
			tokLines2[count] = ln
			--[[
			printf("%5d %s%s |%s|",
				#tokTypes,
				TOKEN_TITLES[tokType],
				(" "):rep(11-#TOKEN_TITLES[tokType]),
				tostring(tokValue))
			--]]
		end
	end

	if expectWordAfterEscape then
		errorInFile(s, path, ptr, "Tokenizer", "Expected a word after '\\'.")
	end

	tokens.count = count

	if increaseTotalLineCount then
		tokens.lineCount = tokens.lineCount + ln - lnStart + 1
	end
end

-- insertToken( tokens, tokenType, tokenValue [, buffer="", path=tokens.file[1] ] )
function _G.insertToken(tokens, tokType, tokValue, buffer, path)
	local count             = tokens.count+1
	tokens.count            = count
	tokens.type [count]     = tokType
	tokens.value[count]     = tokValue
	tokens.position1[count] = 1
	tokens.position2[count] = 1
	tokens.line1[count]     = 1
	tokens.line2[count]     = 1
	tokens.buffer[count]    = buffer or ""
	tokens.file[count]      = path   or tokens.file[1]
	tokens.inserted[count]  = true
end



-- tokenType, tokenValue = getToken( tokens, index )
function _G.getToken(tokens, i)
	return tokens.type[i], tokens.value[i]
end

function _G.isTokenLiteral(tokType, tokValue)
	return
		!!(CONST_SET{ TOKEN_FLOAT, TOKEN_INTEGER, TOKEN_STRING })[tokType]
		or tokType == !(TOKEN_KEYWORD) and !!(CONST_SET{"true","false","nil"})[tokValue] ~= nil
end

function _G.isTokenBuiltinType(tokType, tokValue)
	return tokType == !(TOKEN_KEYWORD) and !!(CONST_SET{"float","int","string","table","bool","Type","any","none"})[tokValue] ~= nil
end

function _G.isTokenAssigning(tokType, tokValue)
	return tokType == !(TOKEN_PUNCTUATION) and !!(CONST_SET{"=","+=","-=","*=","/=","//=","^=","%=","..="})[tokValue] ~= nil
end



do
	local BRACKETS          = {["("]=")", ["{"]="}", ["["]="]"}
	local tokenizeTimeTotal = 0

	function _G.readAndTokenizeString(state, path, increaseTotalLineCount, s, lnStart)
		local tokens     = state.tokens
		local startToken = tokens.count+1

		local startTime = timerGetCurrentInSeconds()
		tokenize(s, path, tokens, increaseTotalLineCount, lnStart)
		local endTime = timerGetCurrentInSeconds()

		!if DEBUG and 1==0 then
			tokenizeTimeTotal = tokenizeTimeTotal + (endTime-startTime)
			printf("+%.3f = %.3f  %s", endTime-startTime, tokenizeTimeTotal, path)
			if path:find("modules/lua.gloa", 1, true) then  os.exit(2)  end
		!end

		-- Quickly check for stray/unmatched brackets.

		-- Note: Sometimes an error message here isn't very helpful, so we
		-- sometimes continue with parsing despite getting an error here.

		local bracketTokenIndices = {}

		for i = startToken, tokens.count do
			if tokens.type[i] == !(TOKEN_PUNCTUATION) and !!(CONST_SET{"(","{","["})[tokens.value[i]] then
				table.insert(bracketTokenIndices, i)

			elseif tokens.type[i] == !(TOKEN_PUNCTUATION) and !!(CONST_SET{")","}","]"})[tokens.value[i]] then
				local iStart   = table.remove(bracketTokenIndices)
				local expected = BRACKETS[tokens.value[iStart]]

				if not iStart then
					reportMessageInFile(io.stderr, s, path, tokens.position1[i], "Error", "BracketChecker", "Stray bracket.")
					bracketTokenIndices[1] = nil
					break

				elseif tokens.value[i] ~= expected then
					printerr()
					reportMessageInFile(io.stderr, s, path, tokens.position1[i],      "Error", "BracketChecker", "Expected '%s'.", expected)
					reportMessageInFile(io.stderr, s, path, tokens.position1[iStart], "Info",  "BracketChecker", "...here is the unmatched bracket.")
					exitFailure()
				end
			end
		end

		if bracketTokenIndices[1] then
			local i = bracketTokenIndices[1]
			errorInFile(s, path, tokens.position1[i], "BracketChecker", "Missing end bracket.", BRACKETS[tokens.value[i]])
		end

		!ifDEBUG `profilerUnpause()`
		return startTime, endTime
	end

	function _G.readAndTokenizeFile(state, path, increaseTotalLineCount)
		!ifDEBUG `profilerPause()`
		assert(not state.fileBuffers[path], path)

		if state.sendMessages then
			local message = newMessage(!(MESSAGE_FILE))
			message.path  = path
			table.insert(state.messages, message)
		end

		local s, err = getFileContents(path, true)
		if not s then
			errorLine("Lexer", "Could not read file '%s'. (%s)", path, err)
		end

		-- Normalize line endings.  @Robustness: Handle the uncommon occurrence of \n\r.
		if s:find("\r", 1, true) then
			s = s:gsub("\r\n?", "\n")
		end

		state.fileBuffers[path] = s

		local startTime, endTime = readAndTokenizeString(state, path, increaseTotalLineCount, s, 1)
		return startTime, endTime
	end
end


