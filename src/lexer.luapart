--[[============================================================
--=
--=  Lexer
--=
--=-------------------------------------------------------------
--=
--=  Glóa - a language that compiles into Lua
--=  by Marcus 'ReFreezed' Thunström
--=
--==============================================================

	insertNewToken, markTokensAsUnused
	isTokenAssigning
	tokenizeString, readAndTokenizeFile

--============================================================]]
!recordLineNumber(@file, @line)

!(
-- Keyword notes:
--   elseif: This actually gets split up into 'else if' or 'else !if'.
--   NULL:   NULL should be used in situations where the actual value doesn't matter, as a dummy value.
--   void:   Like nil represents the absence of a value, void represents the absence of a type (or anything at all as all things must have a type).
local KEYWORDS = Set{
	"and",
	"break",
	"case",
	"cast",
	"continue",
	"defer",
	"do",
	"else",
	"elseif",
	"enum",
	"export",
	"export_value",
	"external",
	"for",
	"if",
	"in",
	"inline",
	"local",
	"namespace",
	"not",
	"or",
	"return",
	"static",
	"struct",
	"type_info",
	"type_of",
	"using",
	"variant_of",
	"while",
	-- Reserved words:
	"goto",
	"read_only",
	"until",
	-- Literals:
	"false",
	"nil",
	"NULL",
	"true",
	-- Built-in types:
	"any",
	"bool",
	"float",
	"int",
	"none",
	"string",
	"table",
	"Type",
	"void", -- Special indicator. Not an actual type.
}

local DIRECTIVE_WORDS = Set{
	"assert",         -- For static/compile-time asserts. Can exist in both imperative and declarative scopes.
	"bake_arguments", -- For "baking" function arguments.
	"body_text",      -- For generating the body of a function at compile-time.
	"call",           -- For defining what happens when calling a struct.
	"caller_location",-- For getting the source code location of a call.
	"char",           -- For getting the Unicode codepoint for a character.
	"compiler",       -- For compiler-provided literal values.
	"complete",       -- For making sure every value of an enum is handled in a switch statement.
	"foreign",        -- For externally-defined values.
	"if",             -- For static/compile-time if statements. Can exist in both imperative and declarative scopes.
	"import",         -- For importing modules/libraries from a common folder. (Related to !load)
	"iterator",       -- For defining default iterators for structs.
	"key",            -- For table-like structs. (Related to !value)
	"load",           -- For loading Glóa files by path. (Related to !import)
	"location",       -- For getting a source code location.
	"must",           -- For return arguments that must be received by the caller (e.g. file handles).
	"preload",        -- For running Lua code before any Glóa code runs.
	"print",          -- For printing out what some expression refers to during compilation. (For debugging)
	"run",            -- For running code at compile-time.
	"self",           -- For a file to reference itself.
	"shadow",         -- For allowing name shadowing (which is normally disallowed) for a declared name.
	"through",        -- For joining cases in switch statements.
	"value",          -- For table-like or array-like structs.
	-- May or may not implement:
	"modify",         -- For modifying declared functions and structs. (Related to !body_text)
	"operator",       -- For overloading operators.
}
)

local KEYWORDS        = !(KEYWORDS)
local DIRECTIVE_WORDS = !(DIRECTIVE_WORDS)



_G.!struct"Tokens"{
	{`n`,          0}, -- Length of array.
	{`nextUnused`, 1},
	{`lastUnused`, 0},
	{`reuseCount`, COUNT_TOKEN_REUSES and 0 or STRUCT_FIELD_IGNORE},
	-- [1]=tok, ...

	-- tok = {
	--   type      = tokenType,  -- One of TOKEN_*.
	--   value     = tokenValue, -- Depends on the type.
	--   extra     = extraValue, -- Depends on the type.
	--   source    = { path=sourcePath, string=sourceString },
	--   position1 = startBytePosition,
	--   position2 = endBytePosition,
	--   line1     = startLineNumber,
	--   line2     = endLineNumber|nil, -- If this is nil then line1 should be used.
	-- }
}

!if DEBUG then
	local tokenTestCounts = {}
	local function increaseTokenTestCount(what)
		tokenTestCounts[what] = (tokenTestCounts[what] or 0) + 1
		return true
	end
	function _G.printTokenTestCounts(what)
		local whats = {}
		for what in pairs(tokenTestCounts) do
			table.insert(whats, what)
		end
		table.sort(whats, function(a, b)
			return tokenTestCounts[a] > tokenTestCounts[b]
		end)
		for _, what in ipairs(whats) do
			print(tokenTestCounts[what], what)
		end
	end
!end
!(
local function TOKEN_TEST(labelCode)
	if DEBUG and 1==0 then  return "increaseTokenTestCount("..labelCode..") and"  end
	return ""
end
)

local function tokenize(state, sourceInfo, lnStart, increaseTotalLineCount)
	local bytesToString = string.char
	local find          = string.find
	local getByte       = string.byte
	local gsub          = string.gsub
	local match         = string.match
	local sub           = string.sub
	local tonumber      = tonumber
	local utf8Char      = utf8Char
	local concat        = table.concat

	local path = sourceInfo.path
	local s    = sourceInfo.string

	local tokens     = state.tokens
	local count      = tokens.n
	local nextUnused = tokens.nextUnused
	local lastUnused = tokens.lastUnused

	!if COUNT_TOKEN_REUSES then
		local reuseCount = tokens.reuseCount
	!end

	local ptr   = 1
	local ln    = lnStart
	local lnPtr = 1
	local lnPos = nil

	local expectWordAfterEscape = false

	!local UPDATE_LINE_NUMBER = `
		while true do
			if not lnPos then
				lnPos = find(s, "\n", lnPtr, true)

				if not lnPos then
					lnPtr = 1/0
					break
				end
			end

			if lnPos >= ptr then
				lnPtr = ptr-1
				break
			end

			ln    = ln+1
			lnPtr = lnPos+1
			lnPos = nil
		end
	`

	while true do
		local b = getByte(s, ptr)

		-- Ignore whitespace.
		if (b == @@BYTE" " or b == @@BYTE"\n" or b == @@BYTE"\t") and not expectWordAfterEscape then
			local _, i2 = find(s, "^%s*", ptr+1)
			ptr         = i2+1
			b           = getByte(s, ptr)

			!!(UPDATE_LINE_NUMBER)
		end

		if ptr > #s then  break  end -- EOF

		local tokenPos = ptr
		local tokType, tokValue, tokExtra = nil

		-- Identifier/keyword/blank.
		!(
		local bytes = {BYTE"_"}
		for b = BYTE"A", BYTE"Z" do  table.insert(bytes, b)  end
		for b = BYTE"a", BYTE"z" do  table.insert(bytes, b)  end
		for b = 194,     244     do  table.insert(bytes, b)  end
		)
		if @@TOKEN_TEST"name" !!(CONST_SET(bytes))[b] then
			local i2

			for _ = 1, 1/0 do
				b = getByte(s, ptr)
				if not b then
					break -- EOF
				elseif b <= 127 then
					_, i2 = find(s, "^[%w_]+", ptr)
				else
					_, i2 = find(s, "^[\194-\244][\128-\191]*", ptr)
				end

				if i2 then
					ptr = i2+1
				else
					break
				end
			end

			local word = sub(s, tokenPos, ptr-1)

			if expectWordAfterEscape then
				expectWordAfterEscape = false
				tokType  = !(TOKEN_IDENTIFIER)
				tokValue = word
			elseif word == "_" then
				tokType  = !(TOKEN_PUNCTUATION)
				tokValue = "_"
			elseif word == "elseif" then
				-- Split up 'elseif' into 'else if'.
				tokType  = !(TOKEN_KEYWORD)
				tokValue = "else"
				ptr      = ptr-2
			else
				tokType  = KEYWORDS[word] and !(TOKEN_KEYWORD) or !(TOKEN_IDENTIFIER)
				tokValue = word
			end

		elseif expectWordAfterEscape then
			errorInFile(s, path, ptr, "Tokenizer", "Expected a word after '\\'.")

		-- Punctuation. (1)
		!local BYTES_EQ = {
			BYTE"+", BYTE"-", BYTE"*", BYTE"/", BYTE"%", BYTE"^",
			BYTE"=", BYTE"~", BYTE"<", BYTE">",
		}
		!local BYTES_SINGLE_UNIQUE = {
			BYTE"#", BYTE"?", BYTE"|", BYTE"$",
			BYTE"(", BYTE")", BYTE"{", BYTE"}", BYTE"]",
			BYTE":", BYTE";", BYTE",",
		}
		!local BYTES_SINGLE_SHARED = {
			BYTE"+", BYTE"-", BYTE"*", BYTE"/", BYTE"%", BYTE"^", BYTE"=", BYTE"<", BYTE">", -- These can be e.g. '+='.
			BYTE"[", -- This can be e.g. '[['.
			BYTE".", -- This can be e.g. '..'.
			BYTE"!", -- This can be e.g. '!foo'.
		}
		elseif @@TOKEN_TEST"punctoneuni" (!!(CONST_SET(BYTES_SINGLE_UNIQUE))[b]) then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = sub(s, ptr, ptr)
			ptr      = ptr+1

		-- Number (hexadecimal, int/float).
		elseif @@TOKEN_TEST"numhex" (b == @@BYTE"0" and find(s, "^[Xx]", ptr+1)) then
			!(
			local NUM_HEX_FRAC_EXP_1 = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_EXP_2 = ("^( .. _*                          %. ([%dA-Fa-f]+) [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_1     = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*) %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_FRAC_2     = ("^( .. _*                          %. ([%dA-Fa-f]+)                                   )"):gsub(" +", "") -- float
			local NUM_HEX_EXP        = ("^( .. _* ([%dA-Fa-f][%dA-Fa-f_]*)                  [Pp]([-+]?[%dA-Fa-f][%dA-Fa-f_]*) )"):gsub(" +", "") -- float
			local NUM_HEX            = ("^( .. _*  [%dA-Fa-f][%dA-Fa-f_]*                                                     )"):gsub(" +", "") -- int
			)
			local           lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_EXP_1), ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_EXP_2), ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_1),     ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_FRAC_2),     ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = true,  false, find(s, !(NUM_HEX_EXP),        ptr)
			if not i1 then  lua52Hex, isInt, i1, i2, numStr = false, true,  find(s, !(NUM_HEX),            ptr)
			if not i1 then  errorInFile(s, path, ptr, "Tokenizer", "Malformed hexadecimal number.")
			end end end end end end

			if find(numStr, "_", 1, true) then  numStr = gsub(numStr, "_+", "")  end
			local n = tonumber(numStr)

			-- Support hexadecimal floats in Lua 5.1.
			if not n and lua52Hex then
				local               _, intStr, fracStr, expStr = match(numStr, !(NUM_HEX_FRAC_EXP_1))
				if not intStr then  _,         fracStr, expStr = match(numStr, !(NUM_HEX_FRAC_EXP_2)) ; intStr                  = ""
				if not intStr then  _, intStr, fracStr         = match(numStr, !(NUM_HEX_FRAC_1))     ;                  expStr =         "0"
				if not intStr then  _,         fracStr         = match(numStr, !(NUM_HEX_FRAC_2))     ; intStr,          expStr = "",     "0"
				if not intStr then  _, intStr,          expStr = match(numStr, !(NUM_HEX_EXP))        ;         fracStr         =     ""
				assert(intStr, numStr)
				end end end end

				n = tonumber(intStr, 16) or 0 -- Note: intStr may be "".

				local fracValue = 1
				for i = 1, #fracStr do
					fracValue = fracValue / 16
					n         = n + tonumber(sub(fracStr, i, i), 16) * fracValue
				end

				n = n * 2 ^ gsub(expStr, "^+", "")
			end

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid hexadecimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- Number (decimal, int/float).
		elseif @@TOKEN_TEST"numdec" ((b >= @@BYTE"0" and b <= @@BYTE"9") or (b == @@BYTE"." and find(s, "^%d", ptr+1))) then
			!(
			local NUM_DEC_FRAC_EXP_1 = ("^( %d[%d_]* %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_EXP_2 = ("^(          %. %d[%d_]* [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_1     = ("^( %d[%d_]* %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_FRAC_2     = ("^(          %. %d[%d_]*                   )"):gsub(" +", "") -- float
			local NUM_DEC_EXP        = ("^( %d[%d_]*             [Ee][-+]?%d[%d_]* )"):gsub(" +", "") -- float
			local NUM_DEC            = ("^( %d[%d_]*                               )"):gsub(" +", "") -- int
			)
			local isInt = false
			local           i1, i2, numStr = find(s, !(NUM_DEC_FRAC_EXP_1), ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_EXP_2), ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_1),     ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_FRAC_2),     ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC_EXP),        ptr)
			if not i1 then  i1, i2, numStr = find(s, !(NUM_DEC),            ptr) ; isInt = true
			if not i1 then  errorInFile(s, path, ptr, "Tokenizer", "Malformed decimal number.")
			end end end end end end

			if find(numStr, "_", 2, true) then  numStr = gsub(numStr, "_+", "")  end
			local n = tonumber(numStr)

			if not n then
				errorInFile(s, path, ptr, "Tokenizer", "Internal compiler error: Invalid decimal number.")
			end

			ptr      = i2+1
			tokType  = isInt and !(TOKEN_INTEGER) or !(TOKEN_FLOAT)
			tokValue = n

		-- String (short-form).
		elseif @@TOKEN_TEST"strshort" (b == @@BYTE'"' or b == @@BYTE"'") then
			local reprStart = ptr
			local quoteByte = getByte(s, ptr)
			local segCount  = 0
			local pattern   = quoteByte == @@BYTE'"' and '["\\]' or "['\\]"

			ptr = ptr + 1 -- The starting quote.

			!(
			preprocessorOutputAtTopOfFile `local substrings = {}` -- Reuse the same table for all strings.

			local function ADD(substrCode)
				return templateToLua(
					trimTemplate`
						segCount             = segCount + 1
						substrings[segCount] = $substr
					`,
					{substr=substrCode}
				)
			end
			)

			while true do
				local i = find(s, pattern, ptr)
				if not i then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string.")
				end

				if i > ptr then
					@@ADD(sub(s, ptr, i-1))
				end
				ptr = i

				b   = getByte(s, ptr)
				ptr = ptr + 1 -- The matching special character.

				-- End of string.
				if b == quoteByte then
					break

				-- Escape sequence.
				else--if b == @@BYTE"\\" then
					b = getByte(s, ptr)

					if not b then
						errorInFile(s, path, reprStart, "Tokenizer", "Unfinished string after escape character.")

					-- \n or \ followed by a newline inserts a newline.
					-- \r inserts a carriage return,
					-- \t inserts a horizontal tab.
					-- \\ inserts a backslash.
					-- \" inserts a quotation mark.
					-- \' inserts an apostrophe.
					elseif b == @@BYTE"n" or b == @@BYTE"\n" then
						@@ADD"\n"
						ptr = ptr + 1
					elseif b == @@BYTE"r" then
						@@ADD"\r"
						ptr = ptr + 1
					elseif b == @@BYTE"t" then
						@@ADD"\t"
						ptr = ptr + 1
					elseif b == @@BYTE"\\" then
						@@ADD"\\"
						ptr = ptr + 1
					elseif b == @@BYTE'"' then
						@@ADD'"'
						ptr = ptr + 1
					elseif b == @@BYTE"'" then
						@@ADD"'"
						ptr = ptr + 1

					-- \ddd specifies a byte denoted by one to three decimal digits.
					elseif find(s, "^%d", ptr) then
						local numStr = match(s, "^%d%d?%d?", ptr)
						b            = tonumber(numStr)
						if b > 255 then
							errorInFile(
								s, path, ptr, "Tokenizer",
								"Invalid escape sequence. (Decimal number '%d' does not fit inside a byte.)",
								b
							)
						end
						ptr = ptr + #numStr
						@@ADD(bytesToString(b))

					-- \xXX specifies a byte denoted by two hexadecimal digits.
					elseif find(s, "^x[%dA-Fa-f][%dA-Fa-f]", ptr) then
						local hex = sub(s, ptr+1, ptr+2)
						ptr       = ptr + 3
						@@ADD(bytesToString(tonumber(hex, 16)))

					-- \u{XXX} inserts the UTF-8 encoding of a Unicode codepoint denoted by a sequence of hexadecimal digits.
					elseif find(s, "^u{[%dA-Fa-f]+}", ptr) then
						-- @Robustness: Validate integer range.
						local hex = match(s, "^[%dA-Fa-f]+", ptr+2)
						ptr       = ptr + 3 + #hex
						@@ADD(utf8Char(tonumber(hex, 16)))

					-- \z skips the following span of whitespace characters.
					elseif b == @@BYTE"z" then
						local i1, i2 = find(s, "^%s*", ptr+1)
						ptr          = i2 + 1

					else
						errorInFile(s, path, ptr-1, "Tokenizer", "Invalid escape sequence.")
					end
				end
			end

			tokType  = !(TOKEN_STRING)
			tokValue = concat(substrings, "", 1, segCount)
			tokExtra = false--trimmedInitialNewline

		-- String (long-form).
		elseif @@TOKEN_TEST"strlong" (b == @@BYTE"[" and find(s, "^=*%[", ptr+1)) then
			local reprStart      = ptr
			local longEqualSigns = match(s, "^%[(=*)%[", ptr)

			ptr = ptr + 2 + #longEqualSigns

			local stringPos1            = ptr
			local trimmedInitialNewline = false

			if getByte(s, stringPos1) == @@BYTE"\n" then
				stringPos1            = stringPos1+1
				trimmedInitialNewline = true
			end

			local i1, i2 = find(s, "%]"..longEqualSigns.."%]", ptr)
			if not i1 then
				errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long string.")
			end
			ptr = i2+1

			local stringPos2 = i1-1

			tokType  = !(TOKEN_STRING)
			tokValue = sub(s, stringPos1, stringPos2)
			tokExtra = trimmedInitialNewline

		-- Comment.
		elseif @@TOKEN_TEST"comment" (b == @@BYTE"-" and find(s, "^%-", ptr+1)) then
			local reprStart = ptr
			ptr = ptr+2

			local longEqualSigns = match(s, "^%[(=*)%[", ptr)

			-- Single line.
			if not longEqualSigns then
				local i = find(s, "\n", ptr)
				ptr     = (i or #s) + 1

			-- Long-form.
			else
				ptr = ptr + 2 + #longEqualSigns

				local i1, i2 = find(s, "%]"..longEqualSigns.."%]", ptr)
				if not i1 then
					errorInFile(s, path, reprStart, "Tokenizer", "Unfinished long comment.")
				end
				ptr = i2+1
			end

			-- Don't set tokType - just ignore the comment!

		-- Directive.
		elseif @@TOKEN_TEST"directive" (b == @@BYTE"!" and find(s, "^%a", ptr+1)) then
			ptr = ptr+1

			local i1, i2, word = find(s, "^(%a[%w_]+)", ptr)
			if not DIRECTIVE_WORDS[word] then
				errorInFile(s, path, ptr, "Tokenizer", "Unknown directive '%s'.", word)
			end
			ptr = i2+1

			tokType  = !(TOKEN_DIRECTIVE)
			tokValue = word

		-- Punctuation. (2)
		elseif @@TOKEN_TEST"punctdotdot" (b == @@BYTE"."             and find(s, "^%.[.=]", ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+2) ; ptr = ptr+3
		elseif @@TOKEN_TEST"punctidiveq" (b == @@BYTE"/"             and find(s, "^/=",     ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+2) ; ptr = ptr+3
		elseif @@TOKEN_TEST"punctconcat" (b == @@BYTE"."             and find(s, "^%.",     ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif @@TOKEN_TEST"punctidiv"   (b == @@BYTE"/"             and find(s, "^/",      ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif @@TOKEN_TEST"punctarrow"  (b == @@BYTE"-"             and find(s, "^>",      ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif @@TOKEN_TEST"puncteq"     (!!(CONST_SET(BYTES_EQ))[b] and find(s, "^=",      ptr+1)) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr+1) ; ptr = ptr+2
		elseif @@TOKEN_TEST"punctonesha" (!!(CONST_SET(BYTES_SINGLE_SHARED))[b]                   ) then  tokType = !(TOKEN_PUNCTUATION) ; tokValue = sub(s, ptr, ptr  ) ; ptr = ptr+1

		elseif @@TOKEN_TEST"note" (b == @@BYTE"@") then
			tokType  = !(TOKEN_PUNCTUATION)
			tokValue = "@"
			ptr      = ptr+1
			if not find(s, "^[%a_\194-\244]", ptr) then
				errorInFile(s, path, ptr, "Tokenizer", "Expected name after '@'.")
			end

		-- Escaped identifier.
		elseif @@TOKEN_TEST"escape" (b == @@BYTE"\\") then
			ptr                   = ptr+1
			expectWordAfterEscape = true
			-- Don't set tokType - just proceed to the next token!

		else
			errorInFile(s, path, ptr, "Tokenizer", "Unknown character. (Byte: %d)", b)
		end

		local tok

		if tokType then
			count = count + 1

			if nextUnused <= lastUnused then
				tok        = tokens[nextUnused]
				nextUnused = nextUnused + 1

				tok.type      = tokType
				tok.value     = tokValue
				tok.extra     = tokExtra
				tok.source    = sourceInfo
				tok.position1 = tokenPos
				tok.position2 = ptr-1
				tok.line1     = ln
				tok.line2     = nil

				!if COUNT_TOKEN_REUSES then
					reuseCount = reuseCount + 1
				!end

			else
				tok = {
					type      = tokType,
					value     = tokValue,
					extra     = tokExtra,
					source    = sourceInfo,
					position1 = tokenPos,
					position2 = ptr-1,
					line1     = ln,
					-- line2  = nil,
				}
			end

			tokens[count] = tok
		end

		local lnPrev = ln
		!!(UPDATE_LINE_NUMBER)

		if tokType then
			if ln > lnPrev then  tok.line2 = ln  end

			!if 1==0 then
				printf("%5d %s%s |%s|",
					#tokTypes,
					TOKEN_TITLES[tokType],
					(" "):rep(11-#TOKEN_TITLES[tokType]),
					tostring(tokValue)
				)
			!end
		end
	end

	if expectWordAfterEscape then
		errorInFile(s, path, ptr, "Tokenizer", "Expected a word after '\\'.")
	end

	tokens.n          = count
	tokens.nextUnused = nextUnused

	!if COUNT_TOKEN_REUSES then
		tokens.reuseCount = reuseCount
	!end

	if increaseTotalLineCount then
		state.lineCount = state.lineCount + ln - lnStart + 1
	end
end

function _G.insertNewToken(tokens, tokType, tokValue, sourceInfo)
	!ASSERT `sourceInfo`

	local nextUnused = tokens.nextUnused
	local tok

	if nextUnused <= tokens.lastUnused then
		tok               = tokens[nextUnused]
		tokens.nextUnused = nextUnused + 1

		tok.type      = tokType
		tok.value     = tokValue
		tok.extra     = nil
		tok.source    = sourceInfo
		tok.position1 = 1
		tok.position2 = 1
		tok.line1     = 1
		tok.line2     = nil

		!if COUNT_TOKEN_REUSES then
			tokens.reuseCount = tokens.reuseCount + 1
		!end

	else
		tok = {
			type      = tokType,
			value     = tokValue,
			-- extra  = nil,
			source    = sourceInfo,
			position1 = 1,
			position2 = 1,
			line1     = 1,
			-- line2  = nil,
		}
	end

	local count   = tokens.n + 1
	tokens.n      = count
	tokens[count] = tok
end

function _G.markTokensAsUnused(state)
	state.tokens.lastUnused = state.tokens.n
	state.nextToken         = state.tokens.n + 1 -- Not sure if actually needed.
end



function _G.isTokenAssigning(tok)
	return tok.type == !(TOKEN_PUNCTUATION) and !!(CONST_SET{"=","+=","-=","*=","/=","//=","^=","%=","..="})[tok.value] ~= nil
end



function _G.tokenizeString(state, sourceInfo, lnStart, increaseTotalLineCount)
	local tokens     = state.tokens
	local startToken = tokens.n + 1

	local startTime = timerGetCurrentInSeconds()
	tokenize(state, sourceInfo, lnStart, increaseTotalLineCount)
	local endTime = timerGetCurrentInSeconds()

	!if DEBUG and 1==0 then
		!STATIC `tokenizeTimeTotal = 0.0`
		tokenizeTimeTotal = tokenizeTimeTotal + (endTime-startTime)
		printf("+%.3f = %.3f  %s", endTime-startTime, tokenizeTimeTotal, sourceInfo.path)
		if sourceInfo.path:find("modules/lua.gloa", 1, true) then  os.exit(2)  end
	!end

	-- Quickly check for stray/unmatched brackets.

	-- Note: Sometimes an error message here isn't very helpful, so we
	-- sometimes continue with parsing despite getting an error here.

	!local bracketsConstName  = CONST{ ["("]=")", ["{"]="}", ["["]="]" }
	local bracketTokenIndices = {}

	for i = startToken, tokens.n do
		if tokens[i].type == !(TOKEN_PUNCTUATION) and !!(CONST_SET{"(","{","["})[tokens[i].value] then
			table.insert(bracketTokenIndices, i)

		elseif tokens[i].type == !(TOKEN_PUNCTUATION) and !!(CONST_SET{")","}","]"})[tokens[i].value] then
			local iStart   = table.remove(bracketTokenIndices)
			local expected = !!(bracketsConstName)[tokens[iStart].value]

			if not iStart then
				reportMessageInFile(io.stderr, sourceInfo.string, sourceInfo.path, tokens[i].position1, "Error", "BracketChecker", "Stray bracket.")
				bracketTokenIndices[1] = nil
				break

			elseif tokens[i].value ~= expected then
				printerr()
				reportMessageInFile(io.stderr, sourceInfo.string, sourceInfo.path, tokens[i     ].position1, "Error", "BracketChecker", "Expected '%s'.", expected)
				reportMessageInFile(io.stderr, sourceInfo.string, sourceInfo.path, tokens[iStart].position1, "Info",  "BracketChecker", "...here is the unmatched bracket.")
				exitFailure()
			end
		end
	end

	if bracketTokenIndices[1] then
		local i = bracketTokenIndices[1]
		errorInFile(sourceInfo.string, sourceInfo.path, tokens[i].position1, "BracketChecker", "Missing end bracket.", !!(bracketsConstName)[tokens[i].value])
	end

	!ifDEBUG `profilerUnpause()`
	return startTime, endTime
end

function _G.readAndTokenizeFile(state, path, increaseTotalLineCount)
	!ifDEBUG `profilerPause()`
	assert(not state.fileBuffers[path], path)

	if state.sendMessages then
		local message = newMessage(!(MESSAGE_FILE))
		message.path  = path
		table.insert(state.messages, message)
	end

	local s, err = getFileContents(path, true)
	if not s then
		errorLine("Lexer", "Could not read file '%s'. (%s)", path, err)
	end

	-- Normalize line endings.
	if s:find("\r", 1, true) then
		s = s:gsub("\n?\r\n?", "\n")
	end

	state.fileBuffers[path] = s

	local sourceInfo         = {path=path, string=s}
	local startTime, endTime = tokenizeString(state, sourceInfo, 1, increaseTotalLineCount)
	return startTime, endTime
end


